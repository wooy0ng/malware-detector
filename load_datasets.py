import torch
import re
import numpy as np

from torch.utils.data import Dataset
from sklearn.feature_extraction import FeatureHasher
from sklearn.model_selection import train_test_split

from model import *
from utils import *


class load_datasets(Dataset):
    def __init__(self, args):
        super(Dataset, self).__init__()

        malware_files = get_files_name(args.malware_dir)
        benignware_files = get_files_name(args.benignware_dir)

        hasher = FeatureHasher(30000)

        data = np.asarray(
            [get_string_features(path, hasher) for path in malware_files + benignware_files]
        )
        labels = np.asarray(
            [1 for _ in range(len(malware_files))] + [0 for _ in range(len(benignware_files))]
        )

        datasets = [(d, label) for d, label in zip(data, labels)]
        train_set, test_set = train_test_split(datasets, test_size=0.2, random_state=42)
        if args.mode == 'train':
            self.datasets = train_set
        else:
            self.datasets = test_set

        
    def __len__(self):
        return len(self.datasets)

    def __getitem__(self, idx):
        data = self.datasets[idx][0]
        label = self.datasets[idx][1]

        return data, label
    
    def size(self):
        return self.datasets[0][0].shape[0]


def get_string_features(path, hasher):
    regex = r'[ -~]{5,}'
    pattern = re.compile(regex)

    # get string
    with open(path, errors='ignore', encoding='ascii') as f:
        b2a_data = f.read()
    strings = pattern.findall(b2a_data)

    # hashing
    string_features = {}
    for string in strings:
        string_features[string] = 1
    
    hashed_features = hasher.transform([string_features])
    hashed_features = hashed_features.todense()
    hashed_features = np.array(hashed_features)
    hashed_features = hashed_features[0]
    
    return hashed_features    
