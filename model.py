import torch
import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim
from sklearn.decomposition import PCA


class PCA_Encoder():
    def __init__(self, n_component):
        self.n_component = n_component
        self.pca = PCA(self.n_component)

    def __call__(self, data):
        out = self.pca.fit_transform(data)
        return out

class Classifier(nn.Module):
    def __init__(self, in_size, device):
        super(Classifier, self).__init__()
        self.in_size = in_size
        self.device = device
        self.losses = []

        # sigmoid -> softmax
        self.clf = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(self.in_size, 1000),
            nn.ReLU(),
            nn.Linear(1000, 2),
        )
        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        out = self.clf(x)
        return out

    def _train(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.long)
        labels = F.one_hot(labels, num_classes=2).to(dtype=torch.float32)

        outputs = self.forward(inputs)
        loss = self.criterion(outputs, labels)
        self.losses.append(loss.item())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    
    def _test(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.long)
        labels = F.one_hot(labels, num_classes=2).to(dtype=torch.float32)

        outputs = self.forward(inputs)
        return outputs
        

class AutoEncoder(nn.Module):
    def __init__(self, in_size, latent_size, device):
        super(AutoEncoder, self).__init__()
        
        self.in_size = in_size
        self.latent_size = latent_size
        
        self.device = device
        self.losses = []

        
        self.encoder = nn.Sequential(
            nn.Linear(self.in_size, 10000),
            nn.ReLU(),
            nn.Linear(10000, self.latent_size),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(3000, 10000),
            nn.ReLU(),
            nn.Linear(10000, self.in_size),
            nn.Sigmoid()
        )

        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)
        self.criterion = nn.MSELoss()

    def forward(self, x):
        self.latent_value = self.encoder(x)
        out = self.decoder(self.latent_value)
        return out

    def train(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.float32)

        outputs = self.forward(inputs)

        loss = self.criterion(outputs, inputs)
        self.losses.append(loss.item())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()



