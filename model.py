import torch
import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim
from sklearn.decomposition import PCA


class PCA_Encoder():
    def __init__(self, n_component):
        self.n_component = n_component
        self.pca = PCA(self.n_component)

    def __call__(self, data):

        out = self.pca.fit_transform(data)
        return out

class Classifier(nn.Module):
    def __init__(self, in_size, device):
        super(Classifier, self).__init__()
        self.in_size = in_size
        self.device = device
        self.losses = []

        self.clf = nn.Sequential(
            nn.Linear(self.in_size, self.in_size),
            nn.ReLU(),
            nn.Linear(self.in_size, 1),
            nn.Sigmoid()
        )
        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)
        self.criterion = nn.BCELoss()

    def forward(self, x):
        out = self.clf(x)
        return out

    def train(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.float32)

        outputs = self.forward(inputs)
        loss = self.criterion(outputs, labels.view(-1, 1))
        self.losses.append(loss.item())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    
    def test(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.float32)

        outputs = self.forward(inputs)
        loss = self.criterion(outputs, labels.view(-1, 1))
        self.losses.append(loss.item())
        

class AutoEncoder(nn.Module):
    def __init__(self, in_size, latent_size, device):
        super(AutoEncoder, self).__init__()
        
        self.in_size = in_size
        self.latent_size = latent_size
        
        self.device = device
        self.losses = []

        def _make_encoder():
            layers = []
            in_size = self.in_size

            while True:
                out_size = in_size - 10000
                if not out_size:
                    break
                layers.append(nn.Linear(in_size, out_size))
                layers.append(nn.ReLU())
                in_size = out_size
            return nn.Sequential(*layers)

        def _make_decoder():
            layers = []
            in_size = self.latent_size  # 10000
            
            while True:
                out_size = in_size + 10000
                if out_size > self.in_size:
                    layers.pop()
                    layers.append(nn.Sigmoid())
                    break
                layers.append(nn.Linear(in_size, out_size))
                layers.append(nn.ReLU())
                in_size = out_size
            return nn.Sequential(*layers)            
        
        self.encoder = _make_encoder()
        
        self.decoder = _make_decoder()
        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)
        self.criterion = nn.MSELoss()

    def forward(self, x):
        self.latent_value = self.encoder(x)
        out = self.decoder(self.latent_value)
        return out

    def train(self, inputs, labels):
        inputs = inputs.to(self.device, dtype=torch.float32)
        labels = labels.to(self.device, dtype=torch.float32)

        outputs = self.forward(inputs)

        loss = self.criterion(outputs, inputs)
        self.losses.append(loss.item())

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()



