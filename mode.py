import torch
import torch.nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from load_datasets import *
from sklearn.model_selection import train_test_split
from model import *
import time
import pickle as pkl

import xgboost as xgb
import numpy as np


def train(args):
    '''
    ## Deep Learning using FNN\n
    batch size : 64
    shuffled : true
    '''
    datasets = load_datasets(args)
    data_loader = DataLoader(
        datasets,
        batch_size=64,
        shuffle=True
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Classifier(datasets.size(), device)
    model.train()

    epochs = 2
    start_time = time.time()
    for epoch in range(epochs):
        model.losses = []
        for data, label in data_loader:
            data = data.to(device, dtype=torch.float32)
            label = label.to(device, dtype=torch.float32)
            model._train(data, label)
        print(f"[{epoch} epoch] mean loss : {sum(model.losses) / len(data_loader):.3f}")
    print(f"model fitted\t({time.time() - start_time}sec)")

    torch.save(model.state_dict(), 'model.pt')
    return

def test(args):
    datasets = load_datasets(args)

    data_loader = DataLoader(
        datasets,
        batch_size=1,
        shuffle=False
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Classifier(datasets.size(), device)
    model.eval()

    model.load_state_dict(torch.load('model.pt'))

    cnt = 0
    target, prediction = [], []
    with torch.no_grad():
        for data, label in data_loader:
            outputs = model._test(data, label)
            outputs = (outputs.squeeze() > 0.5).float()
            label = label.squeeze().item()

            target.append(label)
            prediction.append(outputs.item())

            # print(f"predicted : {outputs}\tactual : {label}")
            if outputs.long().item() == label:
                cnt += 1

    print(f"accuaracy : {(cnt / len(data_loader)) * 100.:.3f}%")
    # eval.evaluation_score.f1_score(prediction, target)    
    # eval.evaluation_score.confusion_matrix(prediction, target)
    # eval.evaluation_score.Roc_curve(prediction, target)

    return


def ml_train(args):
    args.mode = 'train'
    datasets = load_datasets(args)
    
    train_X, test_X = [], []
    train_y, test_y = [], []
    for (train_data, train_label), (test_data, test_label) in zip(datasets.train_set, datasets.test_set):
        train_X.append(train_data); test_X.append(test_data)
        train_y.append(train_label); test_y.append(test_label)
    train_X = np.array(train_X); test_X = np.array(test_X)
    train_y = np.array(train_y); test_y = np.array(test_y)
    
    start_time = time.time()
    model = xgb.XGBClassifier()
    model.fit(train_X, train_y)         # training
    print(f"model fitted\t({time.time() - start_time:.3f}sec)")
    pkl.dump(model, open("ml_model.pkl", "wb+"))

    # pred_y = model.predict(test_X)
    print(f"model score : {model.score(test_X, test_y)*100:.3f}%")
    return

def EDA(args):

    return